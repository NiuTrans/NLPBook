# Natural Language Processing:<br>Neural Networks and Large Language Models

This is a book about neural networks and large language models in NLP. It is intended for anyone interested in NLP and deep learning. Some of the chapters are drawn from our previously published articles (e.g., [Introduction to Transformers: An NLP Perspective](https://arxiv.org/abs/2311.17633) and  [Foundations of Large Language Models](https://arxiv.org/abs/2501.09223)), but we have added significant new content.

## Content

<ul>
<li>Table of Contents [pdf]</li>
<li>Part I: Preliminaries</li>
	<ul>
	<li>Chapter 1: Foundations of Machine Learning [pdf]</li>
	<li>Chapter 2: Foundations of Neural Networks [pdf]</li>
	</ul>
<li>Part II: Basic Models</li>
	<ul>
	<li>Chapter 3: Words and Word Vectors [pdf]</li>
	<li>Chapter 4: Recurrent and Convolutional Sequence Modeling [pdf]</li>
	<li>Chapter 5: Sequence-to-Sequence Modeling [pdf]</li>
	<li>Chapter 6: Transformers [pdf]</li>
	</ul>
<li>Part III: Large Language Models</li>
	<ul>
	<li>Chapter 7: Pre-training [pdf]</li>
	<li>Chapter 8: Generative Models [pdf]</li>
	<li>Chapter 9: Prompting [pdf]</li>
	<li>Chapter 10: Alignment [pdf]</li>
	<li>Chapter 11: Inference [pdf]</li>
	</ul>
</ul>

The complete version containing all the chapters can be found here [pdf].

## Citing This Book

@article{Xiao-and-Zhu:2025NLP,<br>
&ensp;&ensp;&ensp;&ensp;title={Natural Language Processing: Neural Networks and Large Language Models},<br>
&ensp;&ensp;&ensp;&ensp;author={Tong Xiao and Jingbo Zhu},<br>
&ensp;&ensp;&ensp;&ensp;year={2025}<br>
}

## Contact Us

For any issues or comments, please feel free to contact the authors directly via e-mail: xiaotong [at] mail.neu.edu.cn
